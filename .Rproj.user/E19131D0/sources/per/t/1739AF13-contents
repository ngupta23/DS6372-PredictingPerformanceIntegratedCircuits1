---
title: "Model Analysis"
author: "Nikhil Gupta"
date: "`r Sys.time()`"
output:
 html_document:
   toc: true
   toc_float: true
   toc_depth: 3
params:
  output.var: 'y3'
  log.pred: FALSE
  eda: TRUE
  algo.forward: FALSE
  algo.backward: FALSE
  algo.stepwise: FALSE
  algo.LASSO: FALSE
  algo.LARS: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
sessionInfo()
```

```{r include=FALSE}
# https://gist.github.com/smithdanielle/9913897
check.packages <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg))
        install.packages(new.pkg, dependencies = TRUE)
    #sapply(pkg, require, character.only = TRUE)
    sapply(pkg, library, character.only = TRUE) # see comment below in GitHub repo
}

# Usage example
packages<-c("dplyr", "DT", "mosaic", "MASS", "usdm", "tidyverse", "ggplot2", "PerformanceAnalytics", "caTools", "glmnet")
check.packages(packages)

library(dplyr)
library(DT)
library(mosaic)
library(MASS)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(caTools)
library(glmnet)
```

# User Inputs
```{r User Inputs}
output.var = params$output.var
log.pred = params$log.pred
eda = params$eda
algo.forward = params$algo.forward
algo.backward = params$algo.backward
algo.stepwise = params$algo.stepwise
algo.LASSO = params$algo.LASSO
algo.LARS = params$algo.LARS

message("Parameters used for training/prediction: ")
str(params)
```

```{r}
# Setup Labels
# alt.scale.label.name = Alternate Scale variable name
#   - if predicting on log, then alt.scale is normal scale
#   - if predicting on normal scale, then alt.scale is log scale
if (log.pred == TRUE){
  label.names = paste('log.',output.var,sep="")
  alt.scale.label.name = output.var
}
if (log.pred == FALSE){
  label.names = output.var
  alt.scale.label.name = paste('log.',output.var,sep="")
}
```


# Prepare Data

## Read and Clean Features
```{r}
features = read.csv("../../Data/features.csv")
#str(features) 
```


### Checking correlations to evaluate removal of redundant features
```{r Correlation of inputs to each other}
corr.matrix = round(cor(features[sapply(features, is.numeric)]),2)

# filter out only highly correlated variables
threshold = 0.6
corr.matrix.tmp = corr.matrix
diag(corr.matrix.tmp) = 0
high.corr = apply(abs(corr.matrix.tmp) >= threshold, 1, any)
high.corr.matrix = corr.matrix.tmp[high.corr, high.corr]

DT::datatable(corr.matrix)
DT::datatable(high.corr.matrix)
```


### Feature Names
```{r}
feature.names = colnames(features)
drops <- c('JobName')
feature.names = feature.names[!(feature.names %in% drops)]
#str(feature.names)
```

## Read and Clean Labels
```{r}
labels = read.csv("../../Data/labels.csv")
#str(labels)
labels = labels[,c("JobName", output.var)]
summary(labels)
```

## Merge Datasets
```{r}
data <- merge(features, labels, by = 'JobName')
drops <- c('JobName')
data = data[,(!colnames(data) %in% drops)]
#str(data)
```

## Transformations
```{r}
#str(data)
if (log.pred == TRUE){
  data[label.names] = log(data[alt.scale.label.name],10)
  
  drops = c(alt.scale.label.name)
  data = data[!(names(data) %in% drops)]
}
#str(data)
```

## Remove NA Cases
```{r}
data = data[complete.cases(data),]
```

## Check correlation of Label with Featires
```{r Correlation of features to label}
if (eda == TRUE){
  corr.to.label =round(cor(dplyr::select(data,-one_of(label.names)),dplyr::select_at(data,label.names)),4)
  DT::datatable(corr.to.label)
}
```

## Multicollinearity - VIF
```{r vif}
if (eda == TRUE){
  vifDF = usdm::vif(select_at(data,feature.names)) %>% arrange(desc(VIF))
  head(vifDF,10)
}
```


# Exploratory Data Analysis

## Scatterplots
```{r}
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
```

```{r}
if (eda == TRUE){
  hist(data[ ,label.names])
  #hist(data[complete.cases(data),alt.scale.label.name])
}
```

```{r}
# https://stackoverflow.com/questions/24648729/plot-one-numeric-variable-against-n-numeric-variables-in-n-plots
ind.pairs.plot <- function(data, xvars=NULL, yvar)
{
    df <- data
    if (is.null(xvars)) {
        xvars = names(data[which(names(data)!=yvar)])       
    }   

    #choose a format to display charts
    ncharts <- length(xvars) 
    
    for(i in 1:ncharts){    
        plot(df[,xvars[i]],df[,yvar], xlab = xvars[i], ylab = yvar)
    }
}

if (eda == TRUE){
  ind.pairs.plot(data, feature.names, label.names)
}

```


## Feature Engineering
```{r}
if(eda ==FALSE){
  # x18 may need transformations
  plot(data[,'x18'], data[,label.names], main = "Original Scatter Plot vs. x18", ylab = label.names, xlab = 'x18')
  plot(sqrt(data[,'x18']), data[,label.names], main = "Original Scatter Plot vs. sqrt(x18)", ylab = label.names, xlab = 'sqrt(x18)')
  
  # transforming x18
  data$sqrt.x18 = sqrt(data$x18)
  data = dplyr::select(data,-one_of('x18'))
  
  # what about x7, x9?
  # x11 looks like data is at discrete points after a while. Will this be a problem?
}
```


# Modeling


##  Train Test Split

```{r}
data = data[sample(nrow(data)),] # randomly shuffle data
split = sample.split(data[,label.names], SplitRatio = 0.8)

data.train = subset(data, split == TRUE)
data.test = subset(data, split == FALSE)
```

## Common Functions
```{r}
plot.diagnostics <-  function(model, train) {
  plot(model)
  
  residuals = resid(model) # Plotted above in plot(lm.out)
  r.standard = rstandard(model)
  r.student = rstudent(model)

  plot(predict(model,train),r.student,
      ylab="Student Residuals", xlab="Predicted Values", 
      main="Student Residual Plot") 
  abline(0, 0)
  
  plot(predict(model, train),r.standard,
      ylab="Standard Residuals", xlab="Predicted Values", 
      main="Standard Residual Plot") 
  abline(0, 0)
  abline(2, 0)
  abline(-2, 0)
  
  # Histogram
  hist(r.student, freq=FALSE, main="Distribution of Studentized Residuals", 
  xlab="Studentized Residuals", ylab="Density", ylim=c(0,0.5))

  # Create range of x-values for normal curve
  xfit <- seq(min(r.student)-1, max(r.student)+1, length=40)

  # Generate values from the normal distribution at the specified values
  yfit <- (dnorm(xfit))

  # Add the normal curve
  lines(xfit, yfit, ylim=c(0,0.5))
  
}
```

## Setup Formulae
```{r}
n <- names(data.train)
formula <- as.formula(paste(paste(n[n %in% label.names], collapse = " + ")," ~", paste(n[!n %in% label.names], collapse = " + "))) 
grand.mean.formula = as.formula(paste(paste(n[n %in% label.names], collapse = " + ")," ~ 1"))
print(formula)
print(grand.mean.formula)

# Update feature.names because we may have transformed some features
feature.names = n[!n %in% label.names]
```

## Full & Grand Means Model

```{r Full Model}
model.full = lm(formula , data.train)
summary(model.full)
plot.diagnostics(model.full, data.train)
```

```{r}
model.null = lm(grand.mean.formula, data.train)
summary(model.null)
plot.diagnostics(model.null, data.train)
```


## Variable Selection 
http://www.stat.columbia.edu/~martin/W2024/R10.pdf 

### Forward Selection

```{r Forward Selection}
if (algo.forward == TRUE){
  t1 = Sys.time()
  
  model.forward = step(model.null, scope=list(lower=model.null, upper=model.full), direction="forward")
  print(summary(model.forward))
  saveRDS(model.forward,file = "model_forward.rds")
  
  t2 = Sys.time()
  print (paste("Time taken for Forward Selection: ",t2-t1, sep = ""))
  
  plot.diagnostics(model.forward, data.train)
  
}
```

### Backward Elimination
```{r Backward Elimination}
if (algo.backward == TRUE){
  # Takes too much time
  t1 = Sys.time()
  
  model.backward = step(model.full, data = data.train, direction="backward")
  print(summary(model.backward))
  saveRDS(model.forward,file = "model_backward.rds")
  
  t2 = Sys.time()
  print (paste("Time taken for Backward Elimination: ",t2-t1, sep = ""))
  
  plot.diagnostics(model.backward, data.train)
}
```

### Stepwise Selection
```{r Stepwise Selection}
if (algo.stepwise == TRUE){
  t1 = Sys.time()
  
  model.stepwise = step(model.null, scope=list(upper=model.full), data = data.train, direction="both")
  print(summary(model.stepwise))
  saveRDS(model.forward,file = "model_stepwise.rds")
  
  t2 = Sys.time()
  print (paste("Time taken for Stepwise Selection: ",t2-t1, sep = ""))
  
  plot.diagnostics(model.stepwise, data.train)
}
```

### LASSO Selection
```{r LASSO}
if (algo.LASSO == TRUE){
  t1 = Sys.time()

  model.LASSO = cv.glmnet(as.matrix(data.train[,feature.names]), data.train[,label.names], nfolds = 5, standardize = TRUE)  
  summary(model.LASSO)
  
  t2 = Sys.time()
  print (paste("Time taken for LASSO: ",t2-t1, sep = ""))
  
  plot(model.LASSO)
  best_lambda = model.LASSO$lambda.1se
  lasso_coef = model.LASSO$glmnet.fit$beta[ , model.LASSO$glmnet.fit$lambda == best_lambda]
  print (lasso_coef)
  lasso_coef [ abs(lasso_coef) > 0 ]
}
```

```{r}
# summary(model.forward)
# summary(model.stepwise)
```



