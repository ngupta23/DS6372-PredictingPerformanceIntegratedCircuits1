<<<<<<< HEAD
print(model.caret$bestTune)
id = NULL # not really needed but added for consistency
return(list(model = model.caret,id = id))
}
}
set.seed(1)
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "leapSeq"
,feature.names = feature.names)
train.caret.glmselect = function(formula, data, method
,subopt = NULL, feature.names
, train.control = NULL, tune.grid = NULL){
if(is.null(train.control)){
train.control <- trainControl(method = "cv"
,number = 10
,search = "grid"
,verboseIter = TRUE
,allowParallel = TRUE
)
}
if(is.null(tune.grid)){
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
tune.grid = data.frame(nvmax = 1:length(feature.names))
}
if (method == 'glmnet' && subopt == 'LASSO'){
# Will only show 1 Lambda value during training, but that is OK
# https://stackoverflow.com/questions/47526544/why-need-to-tune-lambda-with-carettrain-method-glmnet-and-cv-glmnet
lambda = 10^seq(-2,0, length =100)
alpha = c(1)
tune.grid = expand.grid(alpha = alpha,lambda = lambda)
}
if (method == 'glmnet' && subopt == 'LARS'){
}
}
# http://sshaikh.org/2015/05/06/parallelize-machine-learning-in-r-with-multi-core-cpus/
cl <- makeCluster(detectCores()*0.75) # use 75% of cores only, leave rest for other tasks
registerDoParallel(cl)
set.seed(1)
model.caret <- caret::train(formula
, data = data
, method = method
, tuneGrid = tune.grid
, trControl = train.control
)
stopCluster(cl)
registerDoSEQ() # register sequential engine in case you are not using this function anymore
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
print(model.caret$results) # all model results
print(model.caret$bestTune) # best model
model = model.caret$finalModel
# Provides the coefficients of the best model
id = rownames(model.caret$bestTune)
message("Coefficients of final model:")
print (coef(model, id = id))
# Need to find alternate to plotting diagnostic plots
# plot.diagnostics(model.forward,data.train)
# plot(model.forward,labels = colnames(data.train),scale=c("bic")) ## too many variables
return(list(model = model,id = id))
}
if (method == 'glmnet' && subopt == 'LASSO'){
print(model.caret)
print(plot(model.caret))
print(model.caret$bestTune)
id = NULL # not really needed but added for consistency
return(list(model = model.caret,id = id))
}
}
set.seed(1)
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "leapSeq"
,feature.names = feature.names)
stopCluster(cl)
# http://sshaikh.org/2015/05/06/parallelize-machine-learning-in-r-with-multi-core-cpus/
cl <- makeCluster(detectCores()*0.75) # use 75% of cores only, leave rest for other tasks
# http://sshaikh.org/2015/05/06/parallelize-machine-learning-in-r-with-multi-core-cpus/
cl <- makeCluster(detectCores()*0.75) # use 75% of cores only, leave rest for other tasks
registerDoParallel(cl)
registerDoParallel(cl)
stopCluster(cl)
registerDoSEQ() # register sequential engine in case you are not using this function anymore
set.seed(1)
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "leapSeq"
,feature.names = feature.names)
model.stepwise = returned$model
id = returned$id
test.model(model.stepwise, data.test
,method = 'leapSeq',subopt = NULL
,formula = formula, feature.names = feature.names, label.names = label.names
,id = id
,draw.limits = TRUE)
model.stepwise
test.model(model.stepwise, data.test
,method = 'leapSeq',subopt = NULL
,formula = formula, feature.names = feature.names, label.names = label.names
,id = id
,draw.limits = TRUE)
test.model()
id
test.model = function(model, test, level=0.95
,draw.limits = FALSE, good = 0.1, ok = 0.15
,method = NULL, subopt = NULL
,id = NULL, formula, feature.names, label.names){
## if using caret for glm select equivalent functionality,
## need to set regsubset = TRUE, pass id of best model through id variable,
## and pass formula (full is ok as it will select subset of variables from there)
if (is.null(method)){
pred = predict(model, newdata=test, interval="confidence", level = level)
}
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
pred = predict.regsubsets(model, newdata = test, id = id, formula = formula)
}
if (method == 'glmnet' && subopt == 'LASSO'){
xtest = as.matrix(test[,feature.names])
pred=as.data.frame(predict(model, xtest))
print (str(pred))
}
# Summary of predicted values
print ("Summary of predicted values: ")
print(summary(pred[,1]))
test.mse = mean((test[,label.names]-pred[,1])^2)
print (paste(method, subopt, "Test MSE:", test.mse, sep=" "))
plot(test[,label.names],pred[,1],xlab = "Actual", ylab = "Predicted")
abline(0,(1+good),col='green', lwd = 3)
abline(0,(1-good),col='green', lwd = 3)
abline(0,(1+ok),col='blue', lwd = 3)
abline(0,(1-ok),col='blue', lwd = 3)
}
# test.model(model.stepwise, data.test, "Stepwise Selection", draw.limits = TRUE, regsubset = TRUE, id = id, formula = formula)
test.model(model.stepwise, data.test
,method = 'leapSeq',subopt = NULL
,formula = formula, feature.names = feature.names, label.names = label.names
,id = id
,draw.limits = TRUE)
model.stepwise
resid(model.stepwise)
varImp(model.stepwise)
model.stepwise$ress
model.stepwise$np
model.stepwise$nrbar
model.stepwise$d
model.stepwise$rbar
model.stepwise$rss
model.stepwise$nbest
model.stepwise$tuneValue
model.stepwise$nullrss
model.stepwise$sserr
model.stepwise$method
if (algo.LASSO.caret == TRUE){
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "glmnet"
,subopt = 'LASSO'
,feature.names = feature.names)
model.LASSO.caret = returned$model
}
if (algo.LASSO.caret == TRUE){
test.model(model.LASSO.caret, data.test
,method = 'glmnet',subopt = "LASSO"
,formula = formula, feature.names = feature.names, label.names = label.names
,draw.limits = TRUE)
}
rctrl1 <- trainControl(method = "cv", number = 3, returnResamp = "all")
set.seed(849)
test_reg_cv_model <- train(formula, method = "lars", trControl = rctrl1,
preProc = c("center", "scale"))
rctrl1 <- trainControl(method = "cv", number = 3, returnResamp = "all")
set.seed(849)
test_reg_cv_model <- train(formula, data = data.train, method = "lars", trControl = rctrl1,
preProc = c("center", "scale"))
test_reg_pred <- predict(test_reg_cv_model, data.test)
test_reg_pred
str(test_reg_pred)
if (algo.LARS.caret == TRUE){
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LASSO.caret = returned$model
}
if (algo.LARS.caret == TRUE){
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LARS.caret = returned$model
}
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LARS.caret = returned$model
str(model.LARS.caret)
train.caret.glmselect = function(formula, data, method
,subopt = NULL, feature.names
, train.control = NULL, tune.grid = NULL, pre.proc = NULL){
if(is.null(train.control)){
train.control <- trainControl(method = "cv"
,number = 10
,search = "grid"
,verboseIter = TRUE
,allowParallel = TRUE
)
}
if(is.null(tune.grid)){
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
tune.grid = data.frame(nvmax = 1:length(feature.names))
}
if (method == 'glmnet' && subopt == 'LASSO'){
# Will only show 1 Lambda value during training, but that is OK
# https://stackoverflow.com/questions/47526544/why-need-to-tune-lambda-with-carettrain-method-glmnet-and-cv-glmnet
lambda = 10^seq(-2,0, length =100)
alpha = c(1)
tune.grid = expand.grid(alpha = alpha,lambda = lambda)
}
if (method == 'glmnet' && subopt == 'lars'){
pre.proc = c("center", "scale")
}
}
# http://sshaikh.org/2015/05/06/parallelize-machine-learning-in-r-with-multi-core-cpus/
cl <- makeCluster(detectCores()*0.75) # use 75% of cores only, leave rest for other tasks
registerDoParallel(cl)
set.seed(1)
# note that the seed has to actually be set just before this function is called
# settign is above just not ensure reproducibility for some reason
model.caret <- caret::train(formula
, data = data
, method = method
, tuneGrid = tune.grid
, trControl = train.control
, preProc = pre.proc
)
stopCluster(cl)
registerDoSEQ() # register sequential engine in case you are not using this function anymore
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
print(model.caret$results) # all model results
print(model.caret$bestTune) # best model
model = model.caret$finalModel
# Provides the coefficients of the best model
id = rownames(model.caret$bestTune)
message("Coefficients of final model:")
print (coef(model, id = id))
# Need to find alternate to plotting diagnostic plots
# plot.diagnostics(model.forward,data.train)
# plot(model.forward,labels = colnames(data.train),scale=c("bic")) ## too many variables
return(list(model = model,id = id))
}
if (method == 'glmnet' && subopt == 'LASSO'){
print(model.caret)
print(plot(model.caret))
print(model.caret$bestTune)
id = NULL # not really needed but added for consistency
return(list(model = model.caret,id = id))
}
if (method == 'lars'){
print(model.caret)
print(plot(model.caret))
print(model.caret$bestTune)
id = NULL # not really needed but added for consistency
return(list(model = model.caret,id = id))
}
}
if (algo.LARS.caret == TRUE){
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LARS.caret = returned$model
}
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LARS.caret = returned$model
train.caret.glmselect = function(formula, data, method
,subopt = NULL, feature.names
, train.control = NULL, tune.grid = NULL, pre.proc = NULL){
if(is.null(train.control)){
train.control <- trainControl(method = "cv"
,number = 10
,search = "grid"
,verboseIter = TRUE
,allowParallel = TRUE
)
}
if(is.null(tune.grid)){
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
tune.grid = data.frame(nvmax = 1:length(feature.names))
}
if (method == 'glmnet' && subopt == 'LASSO'){
# Will only show 1 Lambda value during training, but that is OK
# https://stackoverflow.com/questions/47526544/why-need-to-tune-lambda-with-carettrain-method-glmnet-and-cv-glmnet
lambda = 10^seq(-2,0, length =100)
alpha = c(1)
tune.grid = expand.grid(alpha = alpha,lambda = lambda)
}
if (method == 'lars'){
fraction = seq(0, 1, length = 100)
tune.grid = expand.grid(fraction = fraction)
pre.proc = c("center", "scale")
}
}
# http://sshaikh.org/2015/05/06/parallelize-machine-learning-in-r-with-multi-core-cpus/
cl <- makeCluster(detectCores()*0.75) # use 75% of cores only, leave rest for other tasks
registerDoParallel(cl)
set.seed(1)
# note that the seed has to actually be set just before this function is called
# settign is above just not ensure reproducibility for some reason
model.caret <- caret::train(formula
, data = data
, method = method
, tuneGrid = tune.grid
, trControl = train.control
, preProc = pre.proc
)
stopCluster(cl)
registerDoSEQ() # register sequential engine in case you are not using this function anymore
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
print(model.caret$results) # all model results
print(model.caret$bestTune) # best model
model = model.caret$finalModel
# Provides the coefficients of the best model
id = rownames(model.caret$bestTune)
message("Coefficients of final model:")
print (coef(model, id = id))
# Need to find alternate to plotting diagnostic plots
# plot.diagnostics(model.forward,data.train)
# plot(model.forward,labels = colnames(data.train),scale=c("bic")) ## too many variables
return(list(model = model,id = id))
}
if (method == 'glmnet' && subopt == 'LASSO'){
print(model.caret)
print(plot(model.caret))
print(model.caret$bestTune)
id = NULL # not really needed but added for consistency
return(list(model = model.caret,id = id))
}
if (method == 'lars'){
print(model.caret)
print(plot(model.caret))
print(model.caret$bestTune)
id = NULL # not really needed but added for consistency
return(list(model = model.caret,id = id))
}
}
if (algo.LARS.caret == TRUE){
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LARS.caret = returned$model
}
returned = train.caret.glmselect(formula = formula
,data =  data.train
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LARS.caret = returned$model
model.LARS.caret$method
test.model = function(model, test, level=0.95
,draw.limits = FALSE, good = 0.1, ok = 0.15
,method = NULL, subopt = NULL
,id = NULL, formula, feature.names, label.names){
## if using caret for glm select equivalent functionality,
## need to set regsubset = TRUE, pass id of best model through id variable,
## and pass formula (full is ok as it will select subset of variables from there)
if (is.null(method)){
pred = predict(model, newdata=test, interval="confidence", level = level)
}
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
pred = predict.regsubsets(model, newdata = test, id = id, formula = formula)
}
if (method == 'glmnet' && subopt == 'LASSO'){
xtest = as.matrix(test[,feature.names])
pred=as.data.frame(predict(model, xtest))
}
if (method == 'glmnet' && subopt == 'LASSO'){
pred=predict(model, newdata = test)
}
# Summary of predicted values
print ("Summary of predicted values: ")
print(summary(pred[,1]))
test.mse = mean((test[,label.names]-pred[,1])^2)
print (paste(method, subopt, "Test MSE:", test.mse, sep=" "))
plot(test[,label.names],pred[,1],xlab = "Actual", ylab = "Predicted")
abline(0,(1+good),col='green', lwd = 3)
abline(0,(1-good),col='green', lwd = 3)
abline(0,(1+ok),col='blue', lwd = 3)
abline(0,(1-ok),col='blue', lwd = 3)
}
test.model(model.LARS.caret, data.test
,method = 'lars',subopt = NULL
,formula = formula, feature.names = feature.names, label.names = label.names
,draw.limits = TRUE)
test.model = function(model, test, level=0.95
,draw.limits = FALSE, good = 0.1, ok = 0.15
,method = NULL, subopt = NULL
,id = NULL, formula, feature.names, label.names){
## if using caret for glm select equivalent functionality,
## need to set regsubset = TRUE, pass id of best model through id variable,
## and pass formula (full is ok as it will select subset of variables from there)
if (is.null(method)){
pred = predict(model, newdata=test, interval="confidence", level = level)
}
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
pred = predict.regsubsets(model, newdata = test, id = id, formula = formula)
}
if (method == 'glmnet' && subopt == 'LASSO'){
xtest = as.matrix(test[,feature.names])
pred=as.data.frame(predict(model, xtest))
}
if (method == 'lars'){
pred=predict(model, newdata = test)
}
# Summary of predicted values
print ("Summary of predicted values: ")
print(summary(pred[,1]))
test.mse = mean((test[,label.names]-pred[,1])^2)
print (paste(method, subopt, "Test MSE:", test.mse, sep=" "))
plot(test[,label.names],pred[,1],xlab = "Actual", ylab = "Predicted")
abline(0,(1+good),col='green', lwd = 3)
abline(0,(1-good),col='green', lwd = 3)
abline(0,(1+ok),col='blue', lwd = 3)
abline(0,(1-ok),col='blue', lwd = 3)
}
test.model(model.LARS.caret, data.test
,method = 'lars',subopt = NULL
,formula = formula, feature.names = feature.names, label.names = label.names
,draw.limits = TRUE)
test.model = function(model, test, level=0.95
,draw.limits = FALSE, good = 0.1, ok = 0.15
,method = NULL, subopt = NULL
,id = NULL, formula, feature.names, label.names){
## if using caret for glm select equivalent functionality,
## need to set regsubset = TRUE, pass id of best model through id variable,
## and pass formula (full is ok as it will select subset of variables from there)
if (is.null(method)){
pred = predict(model, newdata=test, interval="confidence", level = level)
}
if (method == 'leapForward' | method == 'leapBackward' | method == 'leapSeq'){
pred = predict.regsubsets(model, newdata = test, id = id, formula = formula)
}
if (method == 'glmnet' && subopt == 'LASSO'){
xtest = as.matrix(test[,feature.names])
pred=as.data.frame(predict(model, xtest))
}
if (method == 'lars'){
pred=as.data.frame(predict(model, newdata = test))
}
# Summary of predicted values
print ("Summary of predicted values: ")
print(summary(pred[,1]))
test.mse = mean((test[,label.names]-pred[,1])^2)
print (paste(method, subopt, "Test MSE:", test.mse, sep=" "))
plot(test[,label.names],pred[,1],xlab = "Actual", ylab = "Predicted")
abline(0,(1+good),col='green', lwd = 3)
abline(0,(1-good),col='green', lwd = 3)
abline(0,(1+ok),col='blue', lwd = 3)
abline(0,(1-ok),col='blue', lwd = 3)
}
test.model(model.LARS.caret, data.test
,method = 'lars',subopt = NULL
,formula = formula, feature.names = feature.names, label.names = label.names
,draw.limits = TRUE)
returned = train.caret.glmselect(formula = formula
,data =  data.train2
,method = "lars"
,subopt = 'NULL'
,feature.names = feature.names)
model.LARS.caret = returned$model
test.model(model.LARS.caret, data.test
,method = 'lars',subopt = NULL
,formula = formula, feature.names = feature.names, label.names = label.names
,draw.limits = TRUE)
=======
cv
plot(cv)
summary(cv)
summary(cv)
lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=10,alpha=1)
lasso$lambda.1se
summary(lasso)
lasso
lambda = lasso$lambda.1se #https://stats.stackexchange.com/questions/70249/feature-selection-model-with-glmnet-on-methylation-data-pn
fit.lm.lasso <- glmnet(x, y, alpha = 0, lambda = lambda)
fit.lm.lasso <- glmnetx(as.matrix(df.train[,predictors]),y=df.train[,target]
, alpha = 0, lambda = lambda)
fit.lm.lasso <- glmnet(as.matrix(df.train[,predictors]),y=df.train[,target]
, alpha = 0, lambda = lambda)
summary(fit.lm.lasso)
fit.lm.lasso <- glmnet(as.matrix(df.train[,predictors]),y=df.train[,target]
, alpha = 1, lambda = lambda)
summary(fit.lm.lasso)
.
lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=10,alpha=1)
lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=10,alpha=1)
lasso
plot(fit.lm.lasso)
lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=5,alpha=0)
lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=5,alpha=0)
lasso
plot(lasso)
lambda = lasso$lambda.1se #https://stats.stackexchange.com/questions/70249/feature-selection-model-with-glmnet-on-methylation-data-pn
lambda
lasso
fit.lm.lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=5,alpha=0)
fit.lm.lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=5,alpha=0)
plot(fit.lm.lasso)
fit.lm.lasso
fit.lm.lasso
lambda = fit.lm.lasso$lambda.1se #https://stats.stackexchange.com/questions/70249/feature-selection-model-with-glmnet-on-methylation-data-pn
fit.lm.lasso$glmnet.fit$beta
predict(fit.lm.lasso,data.complete)
predict(fit.lm.lasso,df)
predict(fit.lm.lasso,s=lambda,newx=df.test)
lambda
lambda
df.test
names(df.test)
fit.lm.lasso
glmnet::predict.cv.glmnet(fit.lm.lasso,s=lambda,newx=as.matrix(df.test[],predictors]))
glmnet::predict.cv.glmnet(fit.lm.lasso,s=lambda,newx=as.matrix(df.test[,predictors]))
glmnet::predict.cv.glmnet(fit.lm.lasso,s='almbda.1se',newx=as.matrix(df.test[,predictors]))
glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.test[,predictors]))
glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]))
MSD=Metrics::mse(df.train$y3log,pred)
pred = glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]))
MSD=Metrics::mse(df.train$y3log,pred)
MSD
MED=Metrics::mse(df.train$y3log,pred)
MSE=Metrics::mse(df.train$y3log,pred)
MSE=Metrics::mse(df.train$y3log,predict(fit.lm.AIC,df.train))
MSE
MSE=Metrics::mse(df.train$y3log,predict(fit.lm,df.train))
MSE
plot(predict,resid(fit.lm.lasso))
plot(pred,resid(fit.lm.lasso))
plot(fit.lm.lasso, which=c(1:3))
plot(fit.lm.lasso)
stud <- rstudent(fit.lm.lasso)
investr::plotFit(fit.lm.lasso,data=df.train,interval=c('both'),level=0.95, extend.range=T,shade=T)
ggPredict(fit.lm.lasso,interactive = FALSE)
ggPredict(pred,interactive = FALSE)
pred
plot(fit.lm.lasso)
summary(fit.lm.AIC) #ajd R2 = 0.2388
summary(fit.lm.lasso)
summary(fit.lm.lasso$glmnet.fit)
pred = glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]), type = 'coefficients')
pred
fit.lm.lasso = cv.glmnet(x=as.matrix(df.train[,predictors]),y=df.train[,target]
,standardize=T,type.measure='mse',nfolds=5,alpha=1)
plot(fit.lm.lasso)
fit.lm.lasso
#fit.lm.lasso
lambda = fit.lm.lasso$lambda.1se #https://stats.stackexchange.com/questions/70249/feature-selection-model-with-glmnet-on-methylation-data-pn
pred = glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]))#, type = 'coefficients')
MSE=Metrics::mse(df.train$y3log,pred)
MSE
plot(pred,resid(fit.lm.lasso))
plot(fit.lm.lasso)
ggplot(pred,resid(fit.lm.lasso))
plot(pred,resid(fit.lm.lasso))
ggplot(aes(x=pred,y=resid(fit.lm.lasso)))
plot(pred,resid(fit.lm.lasso))
plot(fit.lm.lasso)
fit.lm.lasso$cvlo
fit.lm.lasso$glmnet.fit
summary(fit.lm.lasso$glmnet.fit)
MSE
plot(fit.lm.lasso)
glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]), type = 'response')
glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]), type = 'coefficients')
glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]))
pred = glmnet::predict.cv.glmnet(fit.lm.lasso,s='lambda.1se',newx=as.matrix(df.train[,predictors]))#, type = 'coefficients')
head(pred)
MSE=Metrics::mse(df.train$y3log,pred)
MSE
plot(pred,resid(fit.lm.lasso))
fit.lm.lasso$glmnet.fit$dev.ratio
which(fit.lm.lasso$glmnet.fit$lambda == fit.lm.lasso$lambda.1se)
lambdaID=which(fit.lm.lasso$glmnet.fit$lambda ==lambda)
#https://stats.stackexchange.com/questions/70249/feature-selection-model-with-glmnet-on-methylation-data-pn
fit.lm.lasso$glmnet.fit$dev.ratio[lambdaID]
coef(fit.lm.lasso, s = "lambda.1se")
lambdaID=which(fit.lm.lasso$lambda.1se==lambda)
lambdaID
fit.lm.lasso$cvm[lambdaID]
e=fit.lm.lasso$cvm[lambdaID]
r2<-1-e/var(fundm)
r2
r2<-1-e/var(df.train[,target])
r2
e
#https://stats.stackexchange.com/questions/70249/feature-selection-model-with-glmnet-on-methylation-data-pn
fit.lm.lasso$glmnet.fit$dev.ratio[lambdaID]
e=fit.lm.lasso$cvm[lambdaID]
r2<-1-e/var(df.train[,target])
r2
r2
fit.lm.lasso
fit.lm.lasso$glmnet.fit$lambda
lambdaID=which(fit.lm.lasso$glmnet.fit$lambda==lambda)
lambdaID
e=fit.lm.lasso$cvm[lambdaID]
r2<-1-e/var(df.train[,target])
r2
e
1 - fit.lm.lasso$cvm/var(df.train[,target])
plot(fit.lm.lasso$glmnet.fit$lambda==lambda$lambda,rsq)
plot(fit.lm.lasso$lambda)
rsq = 1 - fit.lm.lasso$cvm/var(df.train[,target])
rsq
plot(fit.lm.lasso$lambda,rsq)
rsq[lambdaID]
r2
rsq[lambdaID]
r2s = 1 - fit.lm.lasso$cvm/var(df.train[,target])
plot(fit.lm.lasso$lambda,r2s)
fit.lm.lasso$glmnet.fit$dev.ratio[lambdaID]
r2s
rsq[lambdaID]
#https://stats.stackexchange.com/questions/70249/feature-selection-model-with-glmnet-on-methylation-data-pn
fit.lm.lasso$glmnet.fit$dev.ratio[lambdaID]
df.train.fit = dplyr::select(df.train,-JobName)
fit.lm = lm(data=df.train.fit, formula = y3log ~ .)
summary(fit.lm) # adj. R2 = 0.2248
#residual plot
plot(fit.lm, which=c(1:3))
stud <- rstudent(fit.lm)
hist(stud, freq=FALSE, main="Distribution of Studentized Residuals",
xlab="Studentized Residuals", ylab="Density", ylim=c(0,0.5))
MSE=Metrics::mse(df.train$y3log,predict(fit.lm,df.train))
MSE
fit.lm.AIC = readRDS('fit.lm.AIC.rds')
summary(fit.lm.AIC) #ajd R2 = 0.2388
res=resid(model)
plot(df$y13log,  res)
#chart.Correlation(select(data,-JobName),  pch=21)
t=round(cor(select(data.complete,-one_of(target,'JobName')),select_at(data.complete,target)),4)
t=round(cor(select(data.complete,-one_of('JobName'))),4)
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(MASS)
library(glmnet)
library(investr)
library(ggiraph)
library(ggiraphExtra) #https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.html
feat  = read.csv('../../Data/features.csv')
labels = read.csv('../../Data/labels.csv')
predictors = names(dplyr::select(feat,-JobName))
target = 'y3'
data = inner_join(feat,select_at(labels,c('JobName',target)),by='JobName')
data.cc  = complete.cases(data)
data.notComplete = data[! data.cc,]
data.complete = data[data.cc,]
message('Non-Complete cases: ',nrow(data.notComplete))
feat.cc  = complete.cases(feat)
feat.notComplete = feat[! feat.cc,]
feat.complete = feat[feat.cc,]
message('Non-Complete cases: ',nrow(feat.notComplete))
ggplot(gather(select_at(data,target)), aes(value)) +
geom_histogram(aes(y=..density..),bins = 50,fill='light blue') +
geom_density() +
facet_wrap(~key, scales = 'free',ncol=4)
print(summary(data$x11))
ggplot(gather(select_at(data,'x11')), aes(value)) +
geom_histogram(aes(y=..density..),bins = 50,fill='light blue') +
geom_density() +
facet_wrap(~key, scales = 'free',ncol=4)
ggplot(gather(select_at(data,predictors)), aes(value)) +
geom_histogram(aes(y=..density..),bins = 50,fill='light blue') +
geom_density() +
facet_wrap(~key, scales = 'free',ncol=4)
#chart.Correlation(select(data,-JobName),  pch=21)
t=round(cor(select(data.complete,-one_of(target,'JobName')),select_at(data.complete,target)),4)
target
select(data.complete,-one_of(target,'JobName'))
select(data.complete,-dplyr::one_of(target,'JobName'))
select(data.complete,-tidysellect::one_of(target,'JobName'))
one_of(target,'JobName')
install.packages('tidyselect')
install.packages("tidyselect")
install.packages('tidyselect')
install.packages("tidyselect")
install.packages("tidyselect")
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
library(tidyverse)
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
library(tidyverse)
library(tidyverse)
install.packages("tidyselect")
install.packages('tidyverse')
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(MASS)
library(glmnet)
library(investr)
library(ggiraph)
library(ggiraphExtra) #https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.html
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(MASS)
library(glmnet)
library(investr)
library(ggiraph)
library(ggiraphExtra) #https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.html
feat  = read.csv('../../Data/features.csv')
labels = read.csv('../../Data/labels.csv')
predictors = names(dplyr::select(feat,-JobName))
target = 'y3'
data = inner_join(feat,select_at(labels,c('JobName',target)),by='JobName')
data.cc  = complete.cases(data)
data.notComplete = data[! data.cc,]
data.complete = data[data.cc,]
message('Non-Complete cases: ',nrow(data.notComplete))
feat.cc  = complete.cases(feat)
feat.notComplete = feat[! feat.cc,]
feat.complete = feat[feat.cc,]
message('Non-Complete cases: ',nrow(feat.notComplete))
#chart.Correlation(select(data,-JobName),  pch=21)
t=round(cor(select(data.complete,-one_of(target,'JobName')),select_at(data.complete,target)),4)
select(data.complete,-one_of(target,'JobName'))
select_at(data.complete,-one_of(target,'JobName'))
select(data.complete,-one_of(target,'JobName'))
-one_of(target,'JobName')
one_of(target,'JobName')
tidyselect::one_of(target,'JobName')
tidyselect::one_of(c(target,'JobName'))
target
tidyselect::one_of(c('y','JobName'))
tidyselect::one_of(c('y3','JobName'))
vars_select(data.complete,tidyselect::one_of(c('y3','JobName')))
vars(data.complete,tidyselect::one_of(c('y3','JobName')))
nms <- names(iris)
vars_select(nms, starts_with("Petal"))
tidyselect::vars_select
nms <- names(iris)
tidyselect::vars_select(nms, starts_with("Petal"))
library(tidyselect)
nms <- names(iris)
tidyselect::vars_select(data.complete, one_of(target,'JobName'))
nms <- names(iris)
tidyselect::vars_select(data.complete, one_of(c(target,'JobName')))
tidyselect::vars_select(data.complete, one_of('JobName'))
tidyselect::vars_select(data.complete, tidyselect::one_of('JobName'))
select_all(data.complete,contains('JobName'))
select_all(as_tibble(data.complete),contains('JobName'))
install.packages(c("ggmap", "lme4", "modelr", "partykit", "pbapply", "raster"))
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, starts_with("Petal"))
library(tidyverse)
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, starts_with("Petal"))
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, -starts_with("Petal"))
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, -starts_with("Petal"."Species"))
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, -starts_with(c("Petal"."Species")))
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, -one_of("Petal"."Species"))
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, -one_of("Petal"."Species")
iris <- as_tibble(iris) # so it prints a little nicer
select(iris, -one_of('Petal'.'Species'))
select(iris, -one_of('Petal'.'Species'))
select(iris, one_of('Petal'.'Species'))
select(iris, one_of('Petal','Species'))
select(iris, -one_of('Petal','Species'))
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
install.packages('raster')
knitr::opts_chunk$set(echo = FALSE)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(MASS)
library(glmnet)
library(investr)
library(ggiraph)
library(ggiraphExtra) #https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.html
#chart.Correlation(select(data,-JobName),  pch=21)
t=round(cor(select(data.complete,-one_of(target,'JobName')),select_at(data.complete,target)),4)
#chart.Correlation(select(data,-JobName),  pch=21)
t=round(cor(dplyr::select(data.complete,-one_of(target,'JobName')),select_at(data.complete,target)),4)
#rsq http://myweb.uiowa.edu/pbreheny/7600/s16/notes/2-22.pdf
r2s = 1 - fit.lm.lasso$cvm/var(df.train[,target])
r2s[lambdaID]
knitr::opts_chunk$set(echo = TRUE)
# https://gist.github.com/smithdanielle/9913897
check.packages <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
#sapply(pkg, require, character.only = TRUE)
sapply(pkg, library, character.only = TRUE) # see comment below in GitHub repo
}
# Usage example
packages<-c("dplyr", "DT", "mosaic", "MASS", "usdm", "tidyverse", "ggplot2", "PerformanceAnalytics", "caTools", "glmnet", "caret","leaps","doParallel")
check.packages(packages)
library(dplyr)
library(DT)
library(mosaic)
library(MASS)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(caTools)
library(glmnet)
library(caret)
library(leaps)
library(doParallel)
output.var = params$output.var
transform.abs = params$transform.abs
log.pred = params$log.pred
eda = params$eda
algo.forward = params$algo.forward
algo.backward = params$algo.backward
algo.stepwise = params$algo.stepwise
algo.LASSO = params$algo.LASSO
algo.LARS = params$algo.LARS
algo.forward.caret = params$algo.forward.caret
algo.backward.caret = params$algo.backward.caret
algo.stepwise.caret = params$algo.stepwise.caret
algo.LASSO.caret = params$algo.LASSO.caret
algo.LARS.caret = params$algo.LARS.caret
message("Parameters used for training/prediction: ")
str(params)
# Setup Labels
# alt.scale.label.name = Alternate Scale variable name
#   - if predicting on log, then alt.scale is normal scale
#   - if predicting on normal scale, then alt.scale is log scale
if (log.pred == TRUE){
label.names = paste('log.',output.var,sep="")
alt.scale.label.name = output.var
}
if (log.pred == FALSE){
label.names = output.var
alt.scale.label.name = paste('log.',output.var,sep="")
}
features = read.csv("../../Data/features.csv")
#str(features)
>>>>>>> 1a855d316fbfd23a6d50e530d0904ac17ee5f4ac
knitr::opts_chunk$set(echo = TRUE)
sessionInfo()
# https://gist.github.com/smithdanielle/9913897
check.packages <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
#sapply(pkg, require, character.only = TRUE)
sapply(pkg, library, character.only = TRUE) # see comment below in GitHub repo
}
# Usage example
packages<-c("dplyr", "DT", "mosaic", "MASS", "usdm", "tidyverse", "ggplot2", "PerformanceAnalytics", "caTools", "glmnet", "caret","leaps","doParallel")
check.packages(packages)
<<<<<<< HEAD
# library(dplyr)
# library(DT)
# library(mosaic)
# library(MASS)
# library(usdm) #for multicollinearity
# library(tidyverse)
# library(ggplot2)
# library(PerformanceAnalytics)
# library(caTools)
# library(glmnet)
# library(caret)
# library(leaps)
# library(doParallel)
#This line of code installs the pacman page if you do not have it installed - if you do, it simply loads the package
if(!require(pacman))install.packages("pacman")
pacman::p_load("dplyr", "DT", "mosaic", "MASS", "usdm", "tidyverse", "ggplot2", "PerformanceAnalytics", "caTools", "glmnet", "caret","leaps","doParallel", 'bbplot')
=======
library(dplyr)
library(DT)
library(mosaic)
library(MASS)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(caTools)
library(glmnet)
library(caret)
library(leaps)
library(doParallel)
output.var = params$output.var
transform.abs = params$transform.abs
log.pred = params$log.pred
eda = params$eda
algo.forward = params$algo.forward
algo.backward = params$algo.backward
algo.stepwise = params$algo.stepwise
algo.LASSO = params$algo.LASSO
algo.LARS = params$algo.LARS
algo.forward.caret = params$algo.forward.caret
algo.backward.caret = params$algo.backward.caret
algo.stepwise.caret = params$algo.stepwise.caret
algo.LASSO.caret = params$algo.LASSO.caret
algo.LARS.caret = params$algo.LARS.caret
message("Parameters used for training/prediction: ")
str(params)
# Setup Labels
# alt.scale.label.name = Alternate Scale variable name
#   - if predicting on log, then alt.scale is normal scale
#   - if predicting on normal scale, then alt.scale is log scale
if (log.pred == TRUE){
label.names = paste('log.',output.var,sep="")
alt.scale.label.name = output.var
}
if (log.pred == FALSE){
label.names = output.var
alt.scale.label.name = paste('log.',output.var,sep="")
}
features = read.csv("../../Data/features.csv")
#str(features)
corr.matrix = round(cor(features[sapply(features, is.numeric)]),2)
# filter out only highly correlated variables
threshold = 0.6
corr.matrix.tmp = corr.matrix
diag(corr.matrix.tmp) = 0
high.corr = apply(abs(corr.matrix.tmp) >= threshold, 1, any)
high.corr.matrix = corr.matrix.tmp[high.corr, high.corr]
DT::datatable(corr.matrix)
DT::datatable(high.corr.matrix)
feature.names = colnames(features)
drops <- c('JobName')
feature.names = feature.names[!(feature.names %in% drops)]
#str(feature.names)
labels = read.csv("../../Data/labels.csv")
#str(labels)
labels = labels[,c("JobName", output.var)]
summary(labels)
data <- merge(features, labels, by = 'JobName')
drops <- c('JobName')
data = data[,(!colnames(data) %in% drops)]
#str(data)
if (transform.abs == TRUE){
data[,label.names] = 10^(data[,label.names]/20)
data = filter(data, y3 < 1E7)
}
#str(data)
if (log.pred == TRUE){
data[label.names] = log(data[alt.scale.label.name],10)
drops = c(alt.scale.label.name)
data = data[!(names(data) %in% drops)]
}
#str(data)
data = data[complete.cases(data),]
if (eda == TRUE){
corr.to.label =round(cor(dplyr::select(data,-one_of(label.names)),dplyr::select_at(data,label.names)),4)
DT::datatable(corr.to.label)
}
panel.hist <- function(x, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
# https://stackoverflow.com/questions/24648729/plot-one-numeric-variable-against-n-numeric-variables-in-n-plots
ind.pairs.plot <- function(data, xvars=NULL, yvar)
{
df <- data
if (is.null(xvars)) {
xvars = names(data[which(names(data)!=yvar)])
}
#choose a format to display charts
ncharts <- length(xvars)
for(i in 1:ncharts){
plot(df[,xvars[i]],df[,yvar], xlab = xvars[i], ylab = yvar)
}
}
if (eda == TRUE){
ind.pairs.plot(data, feature.names, label.names)
}
#
# pl <- ggplot(data, aes(x=x18, y = y3))
# pl2 <- pl + geom_point(aes(alpha = 0.1)) # default color gradient based on 'hp'
# print(pl2)
>>>>>>> 1a855d316fbfd23a6d50e530d0904ac17ee5f4ac
