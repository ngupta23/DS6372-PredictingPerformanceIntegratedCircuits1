}
#str(data)
data = data[complete.cases(data),]
if (eda == TRUE){
corr.to.label =round(cor(dplyr::select(data,-one_of(label.names)),dplyr::select_at(data,label.names)),4)
DT::datatable(corr.to.label)
}
if (eda == TRUE){
vifDF = usdm::vif(select_at(data,feature.names)) %>% arrange(desc(VIF))
head(vifDF,10)
}
panel.hist <- function(x, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
if (eda == TRUE){
hist(data[ ,label.names])
#hist(data[complete.cases(data),alt.scale.label.name])
}
# https://stackoverflow.com/questions/24648729/plot-one-numeric-variable-against-n-numeric-variables-in-n-plots
ind.pairs.plot <- function(data, xvars=NULL, yvar)
{
df <- data
if (is.null(xvars)) {
xvars = names(data[which(names(data)!=yvar)])
}
#choose a format to display charts
ncharts <- length(xvars)
for(i in 1:ncharts){
plot(df[,xvars[i]],df[,yvar], xlab = xvars[i], ylab = yvar)
}
}
if (eda == TRUE){
ind.pairs.plot(data, feature.names, label.names)
}
if(eda ==FALSE){
# x18 may need transformations
plot(data[,'x18'], data[,label.names], main = "Original Scatter Plot vs. x18", ylab = label.names, xlab = 'x18')
plot(sqrt(data[,'x18']), data[,label.names], main = "Original Scatter Plot vs. sqrt(x18)", ylab = label.names, xlab = 'sqrt(x18)')
# transforming x18
data$sqrt.x18 = sqrt(data$x18)
data = dplyr::select(data,-one_of('x18'))
# what about x7, x9?
# x11 looks like data is at discrete points after a while. Will this be a problem?
}
data = data[sample(nrow(data)),] # randomly shuffle data
split = sample.split(data[,label.names], SplitRatio = 0.8)
data.train = subset(data, split == TRUE)
data.test = subset(data, split == FALSE)
plot.diagnostics <-  function(model, train) {
plot(model)
residuals = resid(model) # Plotted above in plot(lm.out)
r.standard = rstandard(model)
r.student = rstudent(model)
plot(predict(model,train),r.student,
ylab="Student Residuals", xlab="Predicted Values",
main="Student Residual Plot")
abline(0, 0)
plot(predict(model, train),r.standard,
ylab="Standard Residuals", xlab="Predicted Values",
main="Standard Residual Plot")
abline(0, 0)
abline(2, 0)
abline(-2, 0)
# Histogram
hist(r.student, freq=FALSE, main="Distribution of Studentized Residuals",
xlab="Studentized Residuals", ylab="Density", ylim=c(0,0.5))
# Create range of x-values for normal curve
xfit <- seq(min(r.student)-1, max(r.student)+1, length=40)
# Generate values from the normal distribution at the specified values
yfit <- (dnorm(xfit))
# Add the normal curve
lines(xfit, yfit, ylim=c(0,0.5))
}
n <- names(data.train)
formula <- as.formula(paste(paste(n[n %in% label.names], collapse = " + ")," ~", paste(n[!n %in% label.names], collapse = " + ")))
grand.mean.formula = as.formula(paste(paste(n[n %in% label.names], collapse = " + ")," ~ 1"))
print(formula)
print(grand.mean.formula)
model.full = lm(formula , data.train)
summary(model.full)
plot.diagnostics(model.full, data.train)
model.null = lm(grand.mean.formula, data.train)
summary(model.null)
plot.diagnostics(model.null, data.train)
if (algo.forward == TRUE){
t1 = Sys.time()
model.forward = step(model.null, scope=list(lower=model.null, upper=model.full), direction="forward")
summary(model.forward)
t2 = Sys.time()
print (paste("Time taken for Forward Selection: ",t2-t1, sep = ""))
}
if (algo.backward == TRUE){
# Takes too much time
t1 = Sys.time()
model.backward = step(model.full, data = data.train, direction="backward")
summary(model.backward)
t2 = Sys.time()
print (paste("Time taken for Backward Elimination: ",t2-t1, sep = ""))
}
if (algo.stepwise == TRUE){
t1 = Sys.time()
model.stepwise = step(model.null, scope=list(upper=model.full), data = data.train, direction="both")
summary(model.stepwise)
t2 = Sys.time()
print (paste("Time taken for Stepwise Selection: ",t2-t1, sep = ""))
}
if (algo.LASSO == TRUE){
t1 = Sys.time()
model.LASSO = cv.glmnet(as.matrix(data.train[,feature.names]), data.train[,label.names], nfolds = 5, standardize = TRUE)
summary(model.LASSO)
t2 = Sys.time()
print (paste("Time taken for LASSO: ",t2-t1, sep = ""))
plot(model.LASSO)
best_lambda = model.LASSO$lambda.1se
lasso_coef = model.LASSO$glmnet.fit$beta[ , model.LASSO$glmnet.fit$lambda == best_lambda]
print (lasso_coef)
lasso_coef [ abs(lasso_coef) > 0 ]
}
as.matrix(data.train[,feature.names])
feature.names = n[!n %in% label.names]
feature.names
knitr::opts_chunk$set(echo = TRUE)
sessionInfo()
# https://gist.github.com/smithdanielle/9913897
check.packages <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
#sapply(pkg, require, character.only = TRUE)
sapply(pkg, library, character.only = TRUE) # see comment below in GitHub repo
}
# Usage example
packages<-c("dplyr", "DT", "mosaic", "MASS", "usdm", "tidyverse", "ggplot2", "PerformanceAnalytics", "caTools", "glmnet")
check.packages(packages)
library(dplyr)
library(DT)
library(mosaic)
library(MASS)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(caTools)
library(glmnet)
output.var = params$output.var
log.pred = params$log.pred
eda = params$eda
algo.forward = params$algo.forward
algo.backward = params$algo.backward
algo.stepwise = params$algo.stepwise
algo.LASSO = params$algo.LASSO
algo.LARS = params$algo.LARS
message("Parameters used for training/prediction: ")
str(params)
# Setup Labels
# alt.scale.label.name = Alternate Scale variable name
#   - if predicting on log, then alt.scale is normal scale
#   - if predicting on normal scale, then alt.scale is log scale
if (log.pred == TRUE){
label.names = paste('log.',output.var,sep="")
alt.scale.label.name = output.var
}
if (log.pred == FALSE){
label.names = output.var
alt.scale.label.name = paste('log.',output.var,sep="")
}
features = read.csv("../../Data/features.csv")
#str(features)
corr.matrix = round(cor(features[sapply(features, is.numeric)]),2)
# filter out only highly correlated variables
threshold = 0.6
corr.matrix.tmp = corr.matrix
diag(corr.matrix.tmp) = 0
high.corr = apply(abs(corr.matrix.tmp) >= threshold, 1, any)
high.corr.matrix = corr.matrix.tmp[high.corr, high.corr]
DT::datatable(corr.matrix)
DT::datatable(high.corr.matrix)
feature.names = colnames(features)
drops <- c('JobName')
feature.names = feature.names[!(feature.names %in% drops)]
#str(feature.names)
labels = read.csv("../../Data/labels.csv")
#str(labels)
labels = labels[,c("JobName", output.var)]
summary(labels)
data <- merge(features, labels, by = 'JobName')
drops <- c('JobName')
data = data[,(!colnames(data) %in% drops)]
#str(data)
#str(data)
if (log.pred == TRUE){
data[label.names] = log(data[alt.scale.label.name],10)
drops = c(alt.scale.label.name)
data = data[!(names(data) %in% drops)]
}
#str(data)
data = data[complete.cases(data),]
if (eda == TRUE){
corr.to.label =round(cor(dplyr::select(data,-one_of(label.names)),dplyr::select_at(data,label.names)),4)
DT::datatable(corr.to.label)
}
if (eda == TRUE){
vifDF = usdm::vif(select_at(data,feature.names)) %>% arrange(desc(VIF))
head(vifDF,10)
}
panel.hist <- function(x, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
if (eda == TRUE){
hist(data[ ,label.names])
#hist(data[complete.cases(data),alt.scale.label.name])
}
# https://stackoverflow.com/questions/24648729/plot-one-numeric-variable-against-n-numeric-variables-in-n-plots
ind.pairs.plot <- function(data, xvars=NULL, yvar)
{
df <- data
if (is.null(xvars)) {
xvars = names(data[which(names(data)!=yvar)])
}
#choose a format to display charts
ncharts <- length(xvars)
for(i in 1:ncharts){
plot(df[,xvars[i]],df[,yvar], xlab = xvars[i], ylab = yvar)
}
}
if (eda == TRUE){
ind.pairs.plot(data, feature.names, label.names)
}
if(eda ==FALSE){
# x18 may need transformations
plot(data[,'x18'], data[,label.names], main = "Original Scatter Plot vs. x18", ylab = label.names, xlab = 'x18')
plot(sqrt(data[,'x18']), data[,label.names], main = "Original Scatter Plot vs. sqrt(x18)", ylab = label.names, xlab = 'sqrt(x18)')
# transforming x18
data$sqrt.x18 = sqrt(data$x18)
data = dplyr::select(data,-one_of('x18'))
# what about x7, x9?
# x11 looks like data is at discrete points after a while. Will this be a problem?
}
data = data[sample(nrow(data)),] # randomly shuffle data
split = sample.split(data[,label.names], SplitRatio = 0.8)
data.train = subset(data, split == TRUE)
data.test = subset(data, split == FALSE)
plot.diagnostics <-  function(model, train) {
plot(model)
residuals = resid(model) # Plotted above in plot(lm.out)
r.standard = rstandard(model)
r.student = rstudent(model)
plot(predict(model,train),r.student,
ylab="Student Residuals", xlab="Predicted Values",
main="Student Residual Plot")
abline(0, 0)
plot(predict(model, train),r.standard,
ylab="Standard Residuals", xlab="Predicted Values",
main="Standard Residual Plot")
abline(0, 0)
abline(2, 0)
abline(-2, 0)
# Histogram
hist(r.student, freq=FALSE, main="Distribution of Studentized Residuals",
xlab="Studentized Residuals", ylab="Density", ylim=c(0,0.5))
# Create range of x-values for normal curve
xfit <- seq(min(r.student)-1, max(r.student)+1, length=40)
# Generate values from the normal distribution at the specified values
yfit <- (dnorm(xfit))
# Add the normal curve
lines(xfit, yfit, ylim=c(0,0.5))
}
n <- names(data.train)
formula <- as.formula(paste(paste(n[n %in% label.names], collapse = " + ")," ~", paste(n[!n %in% label.names], collapse = " + ")))
grand.mean.formula = as.formula(paste(paste(n[n %in% label.names], collapse = " + ")," ~ 1"))
print(formula)
print(grand.mean.formula)
# Update feature.names because we may have transformed some features
feature.names = n[!n %in% label.names]
model.full = lm(formula , data.train)
summary(model.full)
plot.diagnostics(model.full, data.train)
model.null = lm(grand.mean.formula, data.train)
summary(model.null)
plot.diagnostics(model.null, data.train)
if (algo.forward == TRUE){
t1 = Sys.time()
model.forward = step(model.null, scope=list(lower=model.null, upper=model.full), direction="forward")
summary(model.forward)
t2 = Sys.time()
print (paste("Time taken for Forward Selection: ",t2-t1, sep = ""))
}
if (algo.backward == TRUE){
# Takes too much time
t1 = Sys.time()
model.backward = step(model.full, data = data.train, direction="backward")
summary(model.backward)
t2 = Sys.time()
print (paste("Time taken for Backward Elimination: ",t2-t1, sep = ""))
}
if (algo.stepwise == TRUE){
t1 = Sys.time()
model.stepwise = step(model.null, scope=list(upper=model.full), data = data.train, direction="both")
summary(model.stepwise)
t2 = Sys.time()
print (paste("Time taken for Stepwise Selection: ",t2-t1, sep = ""))
}
if (algo.LASSO == TRUE){
t1 = Sys.time()
model.LASSO = cv.glmnet(as.matrix(data.train[,feature.names]), data.train[,label.names], nfolds = 5, standardize = TRUE)
summary(model.LASSO)
t2 = Sys.time()
print (paste("Time taken for LASSO: ",t2-t1, sep = ""))
plot(model.LASSO)
best_lambda = model.LASSO$lambda.1se
lasso_coef = model.LASSO$glmnet.fit$beta[ , model.LASSO$glmnet.fit$lambda == best_lambda]
print (lasso_coef)
lasso_coef [ abs(lasso_coef) > 0 ]
as.matrix(data.train[,feature.names])
}
knitr::opts_chunk$set(echo = TRUE)
data = read.csv("corr.csv")
str(data)
data = read.csv("corr.csv". sep = ';')
data = read.csv("corr.csv", sep = ';')
str(data)
cor(data)
cor(data)
knitr::opts_chunk$set(echo = TRUE)
sessionInfo()
# https://gist.github.com/smithdanielle/9913897
check.packages <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
#sapply(pkg, require, character.only = TRUE)
sapply(pkg, library, character.only = TRUE) # see comment below in GitHub repo
}
# Usage example
packages<-c("dplyr", "DT", "mosaic", "MASS", "usdm", "tidyverse", "ggplot2", "PerformanceAnalytics", "caTools", "glmnet")
check.packages(packages)
library(dplyr)
library(DT)
library(mosaic)
library(MASS)
library(usdm) #for multicollinearity
library(tidyverse)
library(ggplot2)
library(PerformanceAnalytics)
library(caTools)
library(glmnet)
output.var = params$output.var
transform.abs = params$transform.abs
log.pred = params$log.pred
eda = params$eda
algo.forward = params$algo.forward
algo.backward = params$algo.backward
algo.stepwise = params$algo.stepwise
algo.LASSO = params$algo.LASSO
algo.LARS = params$algo.LARS
message("Parameters used for training/prediction: ")
str(params)
# Setup Labels
# alt.scale.label.name = Alternate Scale variable name
#   - if predicting on log, then alt.scale is normal scale
#   - if predicting on normal scale, then alt.scale is log scale
if (log.pred == TRUE){
label.names = paste('log.',output.var,sep="")
alt.scale.label.name = output.var
}
if (log.pred == FALSE){
label.names = output.var
alt.scale.label.name = paste('log.',output.var,sep="")
}
features = read.csv("../../Data/features.csv")
#str(features)
corr.matrix = round(cor(features[sapply(features, is.numeric)]),2)
# filter out only highly correlated variables
threshold = 0.6
corr.matrix.tmp = corr.matrix
diag(corr.matrix.tmp) = 0
high.corr = apply(abs(corr.matrix.tmp) >= threshold, 1, any)
high.corr.matrix = corr.matrix.tmp[high.corr, high.corr]
DT::datatable(corr.matrix)
DT::datatable(high.corr.matrix)
feature.names = colnames(features)
drops <- c('JobName')
feature.names = feature.names[!(feature.names %in% drops)]
#str(feature.names)
labels = read.csv("../../Data/labels.csv")
#str(labels)
labels = labels[,c("JobName", output.var)]
summary(labels)
data <- merge(features, labels, by = 'JobName')
drops <- c('JobName')
data = data[,(!colnames(data) %in% drops)]
#str(data)
if (transform.abs == TRUE){
data[,label.names] = 10^(data[,label.names]/20)
}
#str(data)
if (log.pred == TRUE){
data[label.names] = log(data[alt.scale.label.name],10)
drops = c(alt.scale.label.name)
data = data[!(names(data) %in% drops)]
}
#str(data)
summary(data[,label.names])
boxplot(data[,label.names])
boxplot(filter(data, y3 < 1E7)
)
boxplot(filter(data, y3 < 1E7)[, label.names])
boxplot(filter(data, y3 < 6e6)[, label.names])
boxplot(filter(data, y3 < 5e6)[, label.names])
boxplot(filter(data, y3 < 1E7)[, label.names])
histogram(filter(data, y3 < 1E7)[, label.names])
data <- merge(features, labels, by = 'JobName')
drops <- c('JobName')
data = data[,(!colnames(data) %in% drops)]
#str(data)
if (transform.abs == TRUE){
data[,label.names] = 10^(data[,label.names]/20)
data = filter(data, y3 < 1E7)[, label.names]
}
#str(data)
if (log.pred == TRUE){
data[label.names] = log(data[alt.scale.label.name],10)
drops = c(alt.scale.label.name)
data = data[!(names(data) %in% drops)]
}
#str(data)
str(data)
data = data[complete.cases(data),]
data <- merge(features, labels, by = 'JobName')
drops <- c('JobName')
data = data[,(!colnames(data) %in% drops)]
#str(data)
if (transform.abs == TRUE){
data[,label.names] = 10^(data[,label.names]/20)
data = filter(data, y3 < 1E7)
}
#str(data)
if (log.pred == TRUE){
data[label.names] = log(data[alt.scale.label.name],10)
drops = c(alt.scale.label.name)
data = data[!(names(data) %in% drops)]
}
#str(data)
data = data[complete.cases(data),]
panel.hist <- function(x, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
if (eda == TRUE){
hist(data[ ,label.names])
#hist(data[complete.cases(data),alt.scale.label.name])
}
if (eda == TRUE){
histogram(data[ ,label.names])
#hist(data[complete.cases(data),alt.scale.label.name])
}
# https://stackoverflow.com/questions/24648729/plot-one-numeric-variable-against-n-numeric-variables-in-n-plots
ind.pairs.plot <- function(data, xvars=NULL, yvar)
{
df <- data
if (is.null(xvars)) {
xvars = names(data[which(names(data)!=yvar)])
}
#choose a format to display charts
ncharts <- length(xvars)
for(i in 1:ncharts){
plot(df[,xvars[i]],df[,yvar], xlab = xvars[i], ylab = yvar)
}
}
if (eda == TRUE){
ind.pairs.plot(data, feature.names, label.names)
}
pl <- ggplot(data, aes(x=x18, y = y3))
pl2 <- pl + geom_point(aes(color=hp),size=3) # default color gradient based on 'hp'
print(pl2)
pl2 <- pl + geom_point(aes(),size=3) # default color gradient based on 'hp'
pl2 <- pl + geom_point(aes(),size=3) # default color gradient based on 'hp'
print(pl2)
pl2 <- pl + geom_point(aes(alpha = 0.5),size=3) # default color gradient based on 'hp'
pl2 <- pl + geom_point(aes(alpha = 0.5),size=3) # default color gradient based on 'hp'
print(pl2)
pl2 <- pl + geom_point(aes(alpha = 0.1),size=3) # default color gradient based on 'hp'
print(pl2)
pl <- ggplot(data, aes(x=sqrt.x18, y = y3))
pl2 <- pl + geom_point(aes(alpha = 0.1),size=3) # default color gradient based on 'hp'
print(pl2)
pl <- ggplot(data, aes(x=x18, y = y3))
pl2 <- pl + geom_point(aes(alpha = 0.1),size=3) # default color gradient based on 'hp'
print(pl2)
pl2 <- pl + geom_point(aes(alpha = 0.1),size=1) # default color gradient based on 'hp'
print(pl2)
pl2 <- pl + geom_point(aes(alpha = 0.1),size=2) # default color gradient based on 'hp'
print(pl2)
pl2 <- pl + geom_point(aes(alpha = 0.1)) # default color gradient based on 'hp'
print(pl2)
